{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gAtgcfkYsKE8"},"outputs":[],"source":["# Instalar las bibliotecas necesarias si aún no están instaladas\n","# Ejecuta estas líneas de instalación en tu terminal o anótalas en un script separado de instalación\n","# !pip install transformers pandas torch\n","\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import accuracy_score \n","\n","# Configurar el dispositivo\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Usando dispositivo: {device}\")\n","\n","# Cargar el dataset\n","dataset_path = '/ruta/al/archivo/Dataset_Final_Limpio_V2.txt'\n","df = pd.read_csv(dataset_path, sep=',')\n","\n","# Configurar el modelo y el tokenizer T5\n","model_name = 't5-small'  # Seleccionar el modelo preentrenado\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n","\n","df.head()\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data.dropna().reset_index(drop=True)  # Eliminar filas con NaN\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data) * 2  # Duplicamos el tamaño para incluir ambas direcciones\n","\n","    def __getitem__(self, index):\n","        real_index = index // 2    # Ajustar el índice real\n","        is_reverse = index % 2 == 1  # Indica si es una traducción inversa (de español a Mapu)\n","\n","        if is_reverse:\n","            input_text = f\"translate Spanish to Mapudungun: {self.data.iloc[real_index]['Español']}\"\n","            target_text = self.data.iloc[real_index]['Mapudungun']\n","        else:\n","            input_text = f\"translate Mapudungun to Spanish: {self.data.iloc[real_index]['Mapudungun']}\"\n","            target_text = self.data.iloc[real_index]['Español']\n","\n","        input_ids = self.tokenizer.encode(input_text, return_tensors='pt', max_length=self.max_length, truncation=True)\n","        target_ids = self.tokenizer.encode(target_text, return_tensors='pt', max_length=self.max_length, truncation=True)\n","\n","        return {\n","            'input_ids': input_ids.flatten(),\n","            'target_ids': target_ids.flatten()\n","        }\n","\n","# Preparar el dataset con DataLoader y padding dinámico\n","def collate_fn(batch):\n","    input_ids = [item['input_ids'] for item in batch]\n","    target_ids = [item['target_ids'] for item in batch]\n","\n","    # Padding dinámico\n","    input_ids = pad_sequence(input_ids, batch_first=True)\n","    target_ids = pad_sequence(target_ids, batch_first=True)\n","\n","    return {\n","        'input_ids': input_ids,\n","        'target_ids': target_ids\n","    }\n","\n","# Crear DataLoader con collate_fn personalizado\n","dataset = TranslationDataset(df, tokenizer)\n","\n","# Dividir el dataset en entrenamiento y validación\n","total_size = len(dataset)\n","train_size = int(0.8 * total_size) # 80% para entrenamiento\n","val_size = total_size - train_size # 20% para validación\n","\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","\n","# Crear DataLoader para entrenamiento y validación\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n","\n","\n","# funcion para calcular la precisión\n","def calculate_accuracy(predictions, targets):\n","    pred_id = torch.argmax(predictions, dim=-1)\n","    return accuracy_score(targets.cpu().numpy().flatten(), pred_id.cpu().numpy().flatten())\n","\n","# Configurar el optimizador y el scheduler\n","optimizer = AdamW(model.parameters(), lr=1e-4)\n","num_epochs = 10\n","num_training_steps = len(train_dataloader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Entrenamiento del modelo\n","best_val_loss = float('inf')\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    total_accuracy = 0\n","\n","    progress_bar = tqdm(train_dataloader,desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n","\n","    for batch in progress_bar:\n","        optimizer.zero_grad()\n","\n","        input_ids = batch['input_ids'].to(device)\n","        target_ids = batch['target_ids'].to(device)\n","\n","        outputs = model(input_ids=input_ids, labels=target_ids)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        total_loss += loss.item()\n","        accuracy = calculate_accuracy(logits, target_ids)\n","        total_accuracy += accuracy\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        progress_bar.set_postfix({\n","            'loss': f\"{loss.item():.4f}\",\n","            'accuracy': f\"{accuracy:.4f}\",\n","            'learning_rate': f\"{scheduler.get_last_lr()[0]:.6f}\"\n","            })\n","        \n","        avg_loss = total_loss / len(train_dataloader)\n","        avg_accuracy = total_accuracy / len(train_dataloader)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(f\"  Average Loss: {avg_loss:.4f}\")\n","        print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n","        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n","\n","\n","        # Evaluacion en el conjunto de validación\n","        model.eval()\n","        val_loss = 0\n","        val_accuracy = 0\n","        with torch.no_grad():\n","            for batch in val_dataloader:\n","                input_ids = batch['input_ids'].to(device)\n","                target_ids = batch['target_ids'].to(device)\n","\n","                outputs = model(input_ids=input_ids, labels=target_ids)\n","                val_loss += outputs.loss.item()\n","                val_accuracy += calculate_accuracy(outputs.logits, target_ids)\n","        \n","        avg_val_loss = val_loss / len(val_dataloader)\n","        avg_val_accuracy = val_accuracy / len(val_dataloader)\n","        print(f\" Validation Loss: {avg_val_loss:.4f}\")\n","        print(f\" Validation Accuracy: {avg_val_accuracy:.4f}\")\n","\n","        # Guardar el mejor modelo basado en la perdida de validacion\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            torch.save(model.state_dict(), 'best_model.pth')\n","            print(\" Nuevo mejor modelo guardado\")\n","\n","\n","print(\"Entrenamiento completado\")\n","\n","# Guardar el modelo entrenado\n","model.save_pretrained('/ruta/para/guardar/translator_model_final')\n","print(\"Modelo guardado\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
