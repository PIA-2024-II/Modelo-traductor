{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si CUDA está disponible y configurarla\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con los datos de traducción (español-mapudungun)\n",
    "csv_file = 'ruta/a/tu/archivo.csv'  # Cambia la ruta al archivo correspondiente\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Mostrar las primeras filas del dataset para verificar que se cargó correctamente\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una clase personalizada para manejar el dataset de traducción\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, source_column, target_column):\n",
    "        self.data = data\n",
    "        self.source_column = source_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text = self.data.loc[idx, self.source_column]\n",
    "        target_text = self.data.loc[idx, self.target_column]\n",
    "        \n",
    "        source_encoding = tokenizer(\n",
    "            source_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        target_encoding = tokenizer(\n",
    "            target_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].flatten().to(device),\n",
    "            'attention_mask': source_encoding['attention_mask'].flatten().to(device),\n",
    "            'labels': target_encoding['input_ids'].flatten().to(device)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizador y el modelo T5\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el dataset y dividirlo en entrenamiento y validación (80% y 20%)\n",
    "translation_dataset = TranslationDataset(data, 'Español', 'Mapudungun')\n",
    "total_size = len(translation_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(translation_dataset, [train_size, val_size])\n",
    "\n",
    "# Crear DataLoader para entrenamiento y validación\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=10, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=10, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la precisión\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    pred_id = torch.argmax(predictions, dim=-1)\n",
    "    return accuracy_score(targets.cpu().numpy().flatten(), pred_id.cpu().numpy().flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el optimizador y el scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "num_epochs = 2\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "\n",
    "    # Iterar sobre el dataloader de entrenamiento\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_acc += calculate_accuracy(logits, batch['labels'])\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_acc = total_train_acc / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Training Accuracy: {avg_train_acc}')\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            total_val_acc += calculate_accuracy(logits, batch['labels'])\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    avg_val_acc = total_val_acc / len(val_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
